---
layout: post
title:  "Class 2: Data Types and Structures "
date:   2015-09-15 18:00:00
categories: "data"
readings: "(1) Data Science for Business, Chapter 1;
(2) [Hadley Wickham on R Data Structures](http://adv-r.had.co.nz/Data-structures.html); (3) OpenTechSchool's *Introduction to Data Processing with Python*  (4) [Data Structures](http://opentechschool.github.io/python-data-intro/core/data.html); (5) [Intro to Pandas](http://www.gregreda.com/2013/10/26/intro-to-pandas-data-structures/)"
assignment: "Lab 1 Due"
permalink: /classes/class2/
header-img: "img/2_header.jpg"
visible: TRUE
---



### Presentation
<iframe src="https://docs.google.com/presentation/d/1blf7JLF6EaWnXrw68HAD2STceeDdUVMX16Hkv099sKE/embed?start=true&loop=false&delayms=3000" frameborder="0" width="960" height="569" allowfullscreen="true" mozallowfullscreen="true" webkitallowfullscreen="true"></iframe>

### Getting Started with Data
The capacity for our society to generate, manage, and use data is incredible.  Data lays at the foundation of any analytics based process.  Without the ability to effectively assess, manage, and interact with data there is little we will be able to do from an analytics perspective. 

### THE CRISP PROCESS MODEL
![title]({{ site.baseurl }}/img/1_crisp.png)
 
Data is the foundation of all analytics processes, but our thinking of how to store and organize data has shift with technology and with the amount of data required to store and analyze.  There are different types of data structures that vary significantly based on their application.  

Insert Slide 3.

### Long Term Persistent Storage
 The first is the *long term persistent storage* solutions that store data on an ongoing basis.  The most common method is through a database management system. While databases used to be primarily relational tables and SQL, they have evolved to meet the varying needs of scale and the applications in which they are deployed. There are key-value, relational, document, triplestore, and graph style database that each provide application relevant specific structures and procedures to map, store, select and extract data for their applications.  Let's go over two of the most common persistent storage types. 

**Relational Databases.**   The most frequent paradigm in IT related system is the relational database.  Databases have enabled the robust organizational and web systems that the cohesive flow of products and services in our society.  Modeling the data within these systems is typically complex, and database management systems (DBMS) have within them structured tables and *structured query language* (SQL) to connect with the tables. These tables individually are matrixes are connected by keys in such a way that they can incorporate the complex nested relationships of objects.

The relational database provides application designers with a way of ensuring consistency while minimizing redundancy. It accomplishes this by separating the data into different tables through a process called *normalization.*  Consider the following example. A customer order is likely to contain multiple different products. Each of these products are similarly likely to have inventory, prior shipments, reviews, etc. In order to minimize the redundancy, many product details are included in a product table and then just linked to other tables through the productID key. This productID key provides think link to show. That way, if there is a change in the name of the product, all applications showing different parts of the workflow would be instantly updated.

![title]({{ site.baseurl }}/img/2_erdiagram.png)

It also means that as data scientists looking to do an analysis of products frequently purchased together one would first have to connect data from a variety of tables. This process of *denormalization* using SQL is outside the scope of this book.   

**Document oriented databases.** The relational database practice of normalization has a number of limitations, however, and document oriented databases have moved in to provide an alternative. 

One limitation of the relational model is it doesn't fit very closely with the object oriented model of programming that has grown to dominate the creation of applications. While software packages called called *object relational mappers* make this process nearly automatic, there is still some complexity in this translation.  In addition, it turns out that as databases scale the process of connecting tables through keys becomes a much more time consuming process. 

As a result, having the capability to store related information in a single place, even if it involves some redundancy, can be a reasonable tradeoff for application developers. In particular,  [MongoDB](https://www.mongodb.com) has emerged in the past 5 years as a real competitor when storing a wide variety of data meant to be presented in web based applications. Rather than linked tables, data is stored in objects which may have a reasonably complex internal hierarchy based on JavaScript Object Nation, which is further discussed below.  

**Application Programming Interfaces.**  For many online sources of data-such as Facebook, LinkedIn, & Twitter, the database structure is hidden from the enduser.  These sites provide Application Programming Interfaces (APIs) that provide standard ways for programmers to authenticate and interact with the applications in different ways.  In many cases, APIs allow both write access to read access to site, enabling developers to create complex applications that work seamlessly with the original.  When working with APIs, data scientists often store the extracted data locally in a file or DBMS so that it can be accessed. 

**Files and Big (Often Unstructured) Data.**   Files are a persistent storage mechanism that works just as well as on your own operating system.  Files can be accessed but there is no additional functionality for things like the selection, indexing, and structure that database management systems provide, and as a result file based storage methods traditionally have not scaled well.   

This has changed though in the context of *big data* storage that has been optimized to do complex analyses on unstructured data.  When people talk about *"big data,"* the majority of the time they are referring to something in the [Hadoop](http://hadoop.apache.org) ecosystem.  Hadoop was birthed at Yahoo, an open source implementation of the Map Reduce algorithm originally developed by Google. At it core, Hadoop was generated for the web--highly unstructured data that had to be processed.  The Hadoop Distributed File System (HDFS) at the core provides a way of storing files in large quantities and the various tools in the Hadoop ecosystem facilitate accessing and processing these files to generate meaningful results.    

*Each of these methods of storing and retrieving data each could require separate courses.  For now, we will assume that you have managed to extract the data into some type of intermediate file format.*

### Intermediate File Structures
Whether you are accessing data from a database, API, or HDFS infrastructure, in many cases you will select the appropriate data and work with it locally for further analysis. This is going to be our entry point to doing data understanding and data preparation.  

**Flat Files**. Files are an extremely common way of interacting with datasets, with comma delimited files being the most common.  Delimited files can be thought of as a structure of a Excel document.  Data within each row is separated by the delimiter.  The delimiter serves to separate the variables and give it structure.  When organized like the example below the data can be easily be opened by programs like Excel or a Google spreadsheet. 

Consider the following [data](https://www.kaggle.com/c/titanic/data) from the [Titanic Kaggle Assignment](https://www.kaggle.com/c/titanic).  Te  
~~~
PassengerId,Survived,Pclass,Name,Sex,Age,SibSp,Parch,Ticket,Fare,Cabin,Embarked
1,0,3,"Braund, Mr. Owen Harris",male,22,1,0,A/5 21171,7.25,,S
2,1,1,"Cumings, Mrs. John Bradley (Florence Briggs Thayer)",female,38,1,0,PC 17599,71.2833,C85,C
3,1,3,"Heikkinen, Miss. Laina",female,26,0,0,STON/O2. 3101282,7.925,,S
4,1,1,"Futrelle, Mrs. Jacques Heath (Lily May Peel)",female,35,1,0,113803,53.1,C123,S
5,0,3,"Allen, Mr. William Henry",male,35,0,0,373450,8.05,,S
6,0,3,"Moran, Mr. James",male,,0,0,330877,8.4583,,Q
7,0,1,"McCarthy, Mr. Timothy J",male,54,0,0,17463,51.8625,E46,S
8,0,3,"Palsson, Master. Gosta Leonard",male,2,3,1,349909,21.075,,S
9,1,3,"Johnson, Mrs. Oscar W (Elisabeth Vilhelmina Berg)",female,27,0,2,347742,11.1333,,S
10,1,2,"Nasser, Mrs. Nicholas (Adele Achem)",female,14,1,0,237736,30.0708,,C
~~~

Notice how text fields are encoded with quotes.  This ensures that if the string has a comma in it there isn't a problem with parsing the document. 

**Javascript Object Notation.**While the flat file is probably the easiest model to work with when loading data, it has some limitations.  The most important one is that it does not have a way of capturing nested data structures.  [Javascript Object Notation (JSON)](http://www.json.org) has emerged as the dominant standard for transmitting data from document/key-value data stores as well APIs.  JSON translators are available for any programming language and can easily input the associated structure into memory. 

~~~
{
  "firstName": "Jane",
  "lastName": "Doe",
  "address": {
    "streetAddress": "1 North Street",
    "city": "New York",
    "state": "NY",
    "postalCode": "10021"
  },
  "phoneNumbers": [
    {
      "type": "home",
      "number": "555-111-2222"
    },
    {
      "type": "mobile",
      "number": "555-121-1212"
    }
  ],
}
~~~

Let's also look at how our titanic data would work with transmitted through JSON.  

~~~
{
  "1": {
    "Survived":"0",
    "Pclass":3,
    "Name":"Braund, Mr. Owen Harris",
    "Sex":"male",
    "Age":22,
    "SibSp":1,
    "Parch":0,
    "Ticket":"A/5 21171",
    "Fare":7.25,
    "Cabin":"",
    "Embarked":"S"
  },
  "2": {
    "Survived":"1",
    "Pclass":1,
    "Name":"Cumings, Mrs. John Bradley (Florence Briggs Thayer)",
    "Sex":"female",
    "Age":38,
    "SibSp":1,
    "Parch":0,
    "Ticket":"PC 17599",
    "Fare":71.2833,
    "Cabin":"C85",
    "Embarked":"C"
  },
  "3": {
    "Survived":"1",
    "Pclass":3,
    "Name":"Heikkinen, Miss. Laina",
    "Sex":"female",
    "Age":26,
    "SibSp":0,
    "Parch":0,
    "Ticket":"STON/O2. 3101282",
    "Fare":7.925,
    "Cabin":"",
    "Embarked":"S"
  },
  "4": {
    "Survived":"1",
    "Pclass":1,
    "Name":"Futrelle, Mrs. Jacques Heath (Lily May Peel)",
    "Sex":"female",
    "Age":35,
    "SibSp":1,
    "Parch":0,
    "Ticket":"113803",
    "Fare":53.1,
    "Cabin":"C123",
    "Embarked":"S"
  },
  "5": {
    "Survived":"0",
    "Pclass":3,
    "Name":"Allen, Mr. William Henry",
    "Sex":"male",
    "Age":35,
    "SibSp":0,
    "Parch":0,
    "Ticket":"373450",
    "Fare":8.05,
    "Cabin":"",
    "Embarked":"S"
  },
  "6": {
    "Survived":"0",
    "Pclass":3,
    "Name":"Moran, Mr. James",
    "Sex":"male",
    "Age":null,
    "SibSp":0,
    "Parch":0,
    "Ticket":"330877",
    "Fare":8.4583,
    "Cabin":"",
    "Embarked":"Q"
  }
}
~~~

Notice how each of the values are nested within the PassengerId field.  This gives a nested relationship to dye data. 

### In Memory Storage for Analysis
When using any programming language, there are both embedded and custom data structures that can be implemented by external packages used in these languages.   These specific characteristics of how data is handled within the programming environment are unique to the languages, meaning that Python, R, and SAS could (and in sometimes do) handle things differently.  At its core though all data are *objects* and thus their behavior is governed by underlying classes. Here we are going to focus on the most common type of data structures for doing data science using R, Python, and SAS and how to go about the process of importing data. In each case, we will start by describing a bit to get started in using the language. 

#### LAB 2 Getting Started with R
R has emerged as among the most popular development environments for analytics.  It has the advantage of being open source and free, has a great community, and it has a tremendous number of packages.  As R is essentially a coding language specific to statistics, it has functional to do just about anything that you want to do.  

Developing with R has made tremendous progress over a short period of time thanks to the developers at RStudio.  RStudio is an Integrated Development Environment (IDE) for R that provides developers with an easy way to manage the development process. To use RStudio on the virtual, first make sure it is running (`vagrant up` to start).  
  
Log onto the RStudio Sever (Virtual machine needs to be running) using the following.
[http://localhost:8787](http://localhost:8787) 
*Username:* vagrant
*Password:* vagrant

Once you log in you will be presented with the IDE console that has everything that you need to get started with data science.  
![title]({{ site.baseurl }}/img/3_console.png)

When you launch RStudio Server, you immediately see view above.  The R console provides a number of tools that can make data science quick and easy.

On the left side you can see the R console.  The R console directly interprets R code and provides answers.  Try it out by doing some basic math.  

~~~
3+3
20/4
20^2
~~~
{: .language-r}

Now assign the results to your first data structure the output of a calculation to a new variable. 

~~~
n<-3+3
~~~
{: .language-r}

Notice that as you entered the variable into the R console, it showed up just to the top right of the screen.  This provides a list of the objects available in memory.  Just type `n` into the console again and get access to the variable and print it out.  

*Tip. Want to resubmit or slightly change a previous entry into the console?  Just press up and then down in order to scroll through previous entries.*  

#### Programs (Scripts) in R.
 While the console may allow you to quickly iterate and test commands, most times you are going to want to develop complex combinations of commands that can enable the transformation and analysis of data.  Anything thing entered into the console is not *persistant*, meaning it is not saved past the initial session.  As a result, we recommend working from a script.  In reality, you can think of a script as a program that you are developing to complete some type of process. Start a new script now by File -> New File -> R Script. 

Copy the earlier commands and add it directly to the script.  You can then add comments to the script and save it for future use with the save button.  
![Scripts]({{ site.baseurl }}/img/3_console.png)

There are three different ways to execute the script.  

* The most common mode of operation is to highlight the code that you would like to execute and then press the *execute* (Run current line or selection).  You can also do this using the shortcut key specific for Mac (command-Enter) or Windows (?).
* After running a selection, you may find that you need to make an edit and rerun the same selection.  The thoughtful individuals at RStudio have made this easy. Through the second execute button (Re-run the previous code region). 
* Finally, to execute the entire file, you can execute the source button (with echo to see it in the console). 

## Vector
A single set of values in a particular order.  Even variables that we have set are really just vectors of length 1. 

We can create a vector using the concatenate function `help(c)` for more. Let's say we want a vector with the ages for 4 students.

~~~
ages<-c(18,19,18,23)
~~~
{: .language-r}

To see  this we can just type the name of object:

~~~
ages
~~~ 
{: .language-r}

To pick a specific value, we can, indicate it. Note. In some languages vectors will start at 0 while in R it will start with 1. 

~~~
ages[4]
~~~ 
{: .language-r}

To specify the range of the vector, it can be clearly indicate.

~~~
ages[2:4]
~~~ 
{: .language-r}

We can have vectors of text, of boolean (TRUE/FALSE), or numbers.

~~~
names<-c("Sally", "Jason", "Bob", "Susy") #Text
female<-c(TRUE, FALSE, FALSE, TRUE)
teacher<-c(Smith, Johnson, Johnson, Smith)
factor(teachers)
grades<-c(20, 15, 13, 19) #25 points possible
~~~ 
{: .language-r}

Vectors can be of several different types.
* **String.** These are the clear character vectors.  (Typically use quotes to add to these vectors.)
* **Numeric. **Numbers in a set.  Note there is not a different type  
* **Boolean. ** TRUE or FALSE values in a set. 
* **Factor.** A situation in which their is a select set of options.  Things such as states or zip codes.  These are typically things which are related to dummy variables, a topic we will discuss later. 

In each of the cases except *factor*, the default data type entered was correct.  In the case 

We can also apply functions to vectors to generate other variables and vectors.

~~~
grades.sum<-sum(grades)
grades.sum
names.length<-nchar(names) 
names.length 
~~~ 
{: .language-r}

#### Using and defining functions in R
R has a wide array of functions that are predefined and can be used in the console.  Before we look at R functions, let's define are own so we understand how they work.  

Let's say we want a function that is ready to add 2 numbers. First we have to *define the function*. 

~~~
#This defines a function called "addTwo."
addTwo <- function(a, b){
c<-a+b
return(c)
}
~~~ 
{: .language-r}

By selecting this function and running it, we can now have access to the function in memory for the rest of this R session.  If we open the script again we would again have to go through the process of loading the function into memory. 

We can then go through the process of utilizing the function by calling it and then specifying the two associated parameters *a & b*. 

~~~
addTwoNumbers(4, 5)
}
~~~ 
{: .language-r}
*Note that the function specification and use is case sensitive.  The use of descriptive functions with all words except the first capitalized in known as [camel case](https://en.wikipedia.org/wiki/CamelCase).*

Organizing processing of data into functions is quite convenient and facilitates what is one of the holy grails of programming: *code reuse*.  By organizing processing of data into functions, you can easily process data for different datasets or subsets of the data without rewriting the steps for each. 

There are lots of [built in functions](http://www.statmethods.net/management/functions.html)  in R that can provide important functionality. Run the following and see:

~~~
x<-abs(-20)
textfield<-toupper("this is a lowercase sentence")
~~~ 
{: .language-r}

To understand each function, we can use the embedded help function `help(abs)` to find out about the *absolute value* function and `help(toupper)` to find out about the *to uppercase* function.


###R Challenge 1. 
In completing this challenge, you should do it in class. 
1. Swap the values of two variables, A and B.  
2. Create a function that returns that average, sum, and standard deviation of a vector.

##Intro to Python
When learning different languages, it is useful to map concepts.  In this case we will be going through the process of learning similar concepts in python that we have just learned in R.  The hope is that you will both be able to be fluent in more than one language and gain a better understanding of the underlying concepts.  

To access iPython, you just need to access another port of your localhost.  Localhost is your machine, and Vagrant has done an excellent job of mapping the ports on your machine (that is the 8001 below) to the appropriate port on the virtual machine.
[http://localhost:8001](http://localhost:8001) 
 
 The virtual machine is configured with [Jupyter](https://jupyter.org), a tool that evolved from the iPython notebook.  You see, a lot of people realized that the specific form of embedding HTML and code that makes up iPython notebooks, can also be useful for a number of other languages.  Jupyter enables use with Python, Julia, R, Haskell, or Ruby.  We will just be using Python in our in initial work.  











### Math and Variables in R
D


##Vectors and Matrixes with R
 




# Getting Started with Python
There are a tremendous wealth of tutorials out there that give you the basics of Python. Data science is quite different from application building through, necessitating not different understanding but different applications.

#IPython Notebook 
http://nbviewer.ipython.org/github/gumption/Python_for_Data_Science/blob/master/Python_for_Data_Science_all.ipynb

http://nbviewer.ipython.org/github/gumption/Python_for_Data_Science/blob/master/Python_for_Data_Science_all.ipynb

Natural Language Toolkit for Python
http://www.nltk.org/book/ 


### Photo Credits
Matrix
https://commons.wikimedia.org/wiki/File%3AMatrix.svg
By Lakeworks (Own work) [GFDL (http://www.gnu.org/copyleft/fdl.html) or CC BY-SA 4.0-3.0-2.5-2.0-1.0 (http://creativecommons.org/licenses/by-sa/4.0-3.0-2.5-2.0-1.0)], via Wikimedia Commons 


### Photo Credits
Header photo by Yale Rosen
Mucoepidermoid carcinoma, high grade Case 200
https://www.flickr.com/photos/pulmonary_pathology/6499879267 

ER Diagram
https://upload.wikimedia.org/wikipedia/commons/7/72/ER_Diagram_MMORPG.png
TheMattrix at the English language Wikipedia [GFDL (www.gnu.org/copyleft/fdl.html) or CC-BY-SA-3.0 (http://creativecommons.org/licenses/by-sa/3.0/)], via Wikimedia Commons

Header photo by Yale Rosen
Mucoepidermoid carcinoma, high grade Case 200
https://www.flickr.com/photos/pulmonary_pathology/6499879267 